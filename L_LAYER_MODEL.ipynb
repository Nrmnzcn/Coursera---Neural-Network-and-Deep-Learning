{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data():\n",
    "    \n",
    "    data = pd.read_csv(\"dataset1.csv\")\n",
    "    x = data.iloc[:, :-1]\n",
    "    x = x.as_matrix()\n",
    "    y = data.iloc[:, -1]\n",
    "    y = y.as_matrix()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L_Layer_Neural_Network:\n",
    "\n",
    "    def __init__(self,layers):\n",
    "        self.layers = layers\n",
    "        self.L = len(self.layers)\n",
    "        self.parameters = {}\n",
    "        self.caches = []\n",
    "\n",
    "    def Sigmoid(self,Z):\n",
    "        cache = Z\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "        return A,cache\n",
    "\n",
    "    def Relu(self,Z):\n",
    "        cache = Z\n",
    "        A = np.maximum(0,Z)\n",
    "        return A,cache\n",
    "\n",
    "    def Sigmoid_Backward(self,dA, cache):\n",
    "        Z = cache\n",
    "        s = 1/(1+np.exp(-Z))\n",
    "        dZ = dA*s*(1-s)\n",
    "        return dZ\n",
    "\n",
    "    def Relu_Backward(self,dA, cache):\n",
    "        Z = cache\n",
    "        dZ = np.array(dA,copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "\n",
    "    def Initialize_Parameters(self):\n",
    "        for l in range(1,self.L):\n",
    "            self.parameters['W'+str(l)] = np.random.randn(self.layers[l],self.layers[l-1])*0.01\n",
    "            self.parameters['b'+str(l)] = np.zeros((self.layers[l],1))\n",
    "\n",
    "            assert(self.parameters['W'+str(l)].shape == (self.layers[l],self.layers[l-1])),\"Parameters W dimension do not match\"\n",
    "            assert(self.parameters['b'+str(l)].shape == (self.layers[l],1)),\"Parameters b dimension do not match\"\n",
    "\n",
    "    def Linear_Forward(self,A,W,b):\n",
    "        Z = np.dot(W,A)+b\n",
    "        assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "        cache = (A,W,b)\n",
    "        return Z,cache\n",
    "\n",
    "    def Linear_Activation_Forward(self,A_prev,W,b,activation):\n",
    "        if activation == 'Sigmoid':\n",
    "            Z, linear_cache = self.Linear_Forward(A_prev,W,b)\n",
    "            A, activation_cache = self.Sigmoid(Z)\n",
    "        elif activation == 'Relu':\n",
    "            Z, linear_cache = self.Linear_Forward(A_prev,W,b)\n",
    "            A, activation_cache = self.Relu(Z)\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        return A, cache\n",
    "\n",
    "    def Forward_Propagation(self,X):\n",
    "        self.caches = []\n",
    "        A = X\n",
    "        L = self.L\n",
    "        m = X.shape[1]\n",
    "        for l in range(1,L-1):\n",
    "            A_prev = A\n",
    "            W = self.parameters['W'+str(l)]\n",
    "            b = self.parameters['b'+str(l)]\n",
    "            A, cache = self.Linear_Activation_Forward(A_prev,W,b,'Relu')\n",
    "            self.caches.append(cache)\n",
    "            \n",
    "        W = self.parameters['W'+str(L-1)]\n",
    "        b = self.parameters['b'+str(L-1)]\n",
    "        AL, cache = self.Linear_Activation_Forward(A,W,b,'Sigmoid')\n",
    "        self.caches.append(cache)\n",
    "        return AL\n",
    "\n",
    "    def Compute_Cost(self,AL):\n",
    "        cost = -1*(np.sum(np.multiply(self.Y,np.log(AL)) + np.multiply(1-self.Y,np.log(1-AL))))/self.m\n",
    "        cost = np.squeeze(cost)\n",
    "        return cost\n",
    "\n",
    "    def Linear_Backward(self,dZ,cache):\n",
    "        A_prev, W, b = cache\n",
    "        dW = (np.dot(dZ,A_prev.T))/self.m\n",
    "        db = np.sum(dZ,axis=1,keepdims=True)/self.m\n",
    "        dA_prev = np.dot(W.T,dZ)\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    def Linear_Activation_Backward(self,dA, cache, activation):\n",
    "        linear_cache, activation_cache = cache\n",
    "        if activation == 'Relu':\n",
    "            dZ = self.Relu_Backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.Linear_Backward(dZ, linear_cache)\n",
    "        if activation == 'Sigmoid':\n",
    "            dZ = self.Sigmoid_Backward(dA, activation_cache)\n",
    "            dA_prev, dW, db = self.Linear_Backward(dZ, linear_cache)\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    def Backward_Propagation(self,AL):\n",
    "        self.grads = {}\n",
    "        dAL = -(np.divide(self.Y,AL) - np.divide(1-self.Y,1-AL))\n",
    "        current_cache = self.caches[self.L-2]\n",
    "        self.grads[\"dA\"+str(self.L-1)], self.grads[\"dW\"+str(self.L-1)], self.grads[\"db\"+str(self.L-1)] = self.Linear_Activation_Backward(dAL,current_cache,\"Sigmoid\")\n",
    "        for l in reversed(range(self.L-2)):\n",
    "            current_cache = self.caches[l]\n",
    "            current_dA = self.grads['dA'+str(l+2)]\n",
    "            # print(len(current_cache))\n",
    "            dA_prev_temp, dW_temp, db_temp = self.Linear_Activation_Backward(current_dA,current_cache,\"Relu\")\n",
    "            self.grads['dA'+str(l+1)] = dA_prev_temp\n",
    "            self.grads['dW'+str(l+1)] = dW_temp\n",
    "            self.grads['db'+str(l+1)] = db_temp\n",
    "\n",
    "    def Update_Parameters(self):\n",
    "        for l in range(1,self.L):\n",
    "            self.parameters[\"W\"+str(l)] = self.parameters[\"W\"+str(l)] - self.learning_rate*self.grads[\"dW\"+str(l)]\n",
    "            self.parameters[\"b\"+str(l)] = self.parameters[\"b\"+str(l)] - self.learning_rate*self.grads[\"db\"+str(l)]\n",
    "\n",
    "    def Train(self,X,Y,learning_rate=0.075,num_iterations=3000,print_cost=False):\n",
    "        self.m = X.shape[0]\n",
    "        self.X = X.T\n",
    "        self.Y = Y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.Initialize_Parameters()\n",
    "        self.costs = []\n",
    "        for i in range(num_iterations):\n",
    "            AL = self.Forward_Propagation(self.X)\n",
    "            cost = self.Compute_Cost(AL)\n",
    "            self.Backward_Propagation(AL)\n",
    "            self.Update_Parameters()\n",
    "            if print_cost and i%100 == 0:\n",
    "                print(\"Cost after iteration \"+str(i)+\": \"+str(cost))\n",
    "                self.costs.append(cost)\n",
    "                # print(self.parameters)\n",
    "        return self.parameters\n",
    "    \n",
    "    def Performance(self, X, Y):\n",
    "        X = np.array(X).T\n",
    "        predictions = self.Forward_Propagation(X)\n",
    "        predictions = np.round(predictions)\n",
    "        print ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6763662016138758\n",
      "Cost after iteration 100: 0.622425323349023\n",
      "Cost after iteration 200: 0.621380154520385\n",
      "Cost after iteration 300: 0.6212450731694789\n",
      "Cost after iteration 400: 0.6211155000098603\n",
      "Cost after iteration 500: 0.6209854041745977\n",
      "Cost after iteration 600: 0.6208546509678115\n",
      "Cost after iteration 700: 0.6207231498365972\n",
      "Cost after iteration 800: 0.6205908014201477\n",
      "Cost after iteration 900: 0.6204574868260516\n",
      "Cost after iteration 1000: 0.620323102308578\n",
      "Cost after iteration 1100: 0.6201874993552342\n",
      "Cost after iteration 1200: 0.6200505667209237\n",
      "Cost after iteration 1300: 0.6199121982189983\n",
      "Cost after iteration 1400: 0.619772138274305\n",
      "Cost after iteration 1500: 0.6196304373124104\n",
      "Cost after iteration 1600: 0.6194868884088895\n",
      "Cost after iteration 1700: 0.6193413399125105\n",
      "Cost after iteration 1800: 0.619193635624633\n",
      "Cost after iteration 1900: 0.6190436158621386\n",
      "Cost after iteration 2000: 0.6188911061043872\n",
      "Cost after iteration 2100: 0.6187359170483613\n",
      "Cost after iteration 2200: 0.6185778612024139\n",
      "Cost after iteration 2300: 0.6184167336640234\n",
      "Cost after iteration 2400: 0.6182523801234897\n",
      "Cost after iteration 2500: 0.6180834677941445\n",
      "Cost after iteration 2600: 0.6179106023108658\n",
      "Cost after iteration 2700: 0.6177350345799205\n",
      "Cost after iteration 2800: 0.617555193314777\n",
      "Cost after iteration 2900: 0.6173708005875261\n",
      "Cost after iteration 3000: 0.617181542228142\n",
      "Cost after iteration 3100: 0.6169868018682709\n",
      "Cost after iteration 3200: 0.6167864023212946\n",
      "Cost after iteration 3300: 0.6165800525411591\n",
      "Cost after iteration 3400: 0.6163673591395673\n",
      "Cost after iteration 3500: 0.6161478832874087\n",
      "Cost after iteration 3600: 0.615919397784575\n",
      "Cost after iteration 3700: 0.6156846026698395\n",
      "Cost after iteration 3800: 0.6154413862877995\n",
      "Cost after iteration 3900: 0.6151888436621256\n",
      "Cost after iteration 4000: 0.6149267738838203\n",
      "Cost after iteration 4100: 0.6146545044876554\n",
      "Cost after iteration 4200: 0.614371342215597\n",
      "Cost after iteration 4300: 0.6140765187710248\n",
      "Cost after iteration 4400: 0.6137692095471159\n",
      "Cost after iteration 4500: 0.6134450494464089\n",
      "Cost after iteration 4600: 0.6131093392780972\n",
      "Cost after iteration 4700: 0.6127582079386772\n",
      "Cost after iteration 4800: 0.6123905264113144\n",
      "Cost after iteration 4900: 0.6120051255177168\n",
      "Cost after iteration 5000: 0.6116006429505015\n",
      "Cost after iteration 5100: 0.6111756572523178\n",
      "Cost after iteration 5200: 0.6107287132497722\n",
      "Cost after iteration 5300: 0.6102581255361998\n",
      "Cost after iteration 5400: 0.6097604550446797\n",
      "Cost after iteration 5500: 0.60923469151764\n",
      "Cost after iteration 5600: 0.6086789567541485\n",
      "Cost after iteration 5700: 0.6080907962667043\n",
      "Cost after iteration 5800: 0.6074675891679119\n",
      "Cost after iteration 5900: 0.6068065173351047\n",
      "Cost after iteration 6000: 0.6061044799770134\n",
      "Cost after iteration 6100: 0.6053576311904221\n",
      "Cost after iteration 6200: 0.6045625052362652\n",
      "Cost after iteration 6300: 0.603714795275326\n",
      "Cost after iteration 6400: 0.6028101269458926\n",
      "Cost after iteration 6500: 0.6018362764987919\n",
      "Cost after iteration 6600: 0.600795082622136\n",
      "Cost after iteration 6700: 0.599673325289755\n",
      "Cost after iteration 6800: 0.5984711107141871\n",
      "Cost after iteration 6900: 0.5971786838928238\n",
      "Cost after iteration 7000: 0.5957958800298278\n",
      "Cost after iteration 7100: 0.5943278423052629\n",
      "Cost after iteration 7200: 0.5927446753197628\n",
      "Cost after iteration 7300: 0.5910355535956553\n",
      "Cost after iteration 7400: 0.5891693300222831\n",
      "Cost after iteration 7500: 0.5871641611057944\n",
      "Cost after iteration 7600: 0.5849927005919368\n",
      "Cost after iteration 7700: 0.5825903439645761\n",
      "Cost after iteration 7800: 0.5800436428838013\n",
      "Cost after iteration 7900: 0.5772715880718241\n",
      "Cost after iteration 8000: 0.5742560125509071\n",
      "Cost after iteration 8100: 0.5709774420214384\n",
      "Cost after iteration 8200: 0.5674033610873304\n",
      "Cost after iteration 8300: 0.5635020580910346\n",
      "Cost after iteration 8400: 0.5591944841637315\n",
      "Cost after iteration 8500: 0.5545439334540451\n",
      "Cost after iteration 8600: 0.5494873699495751\n",
      "Cost after iteration 8700: 0.543960342371663\n",
      "Cost after iteration 8800: 0.5378141349939806\n",
      "Cost after iteration 8900: 0.5311016660050011\n",
      "Cost after iteration 9000: 0.5238741287363675\n",
      "Cost after iteration 9100: 0.5160044775202831\n",
      "Cost after iteration 9200: 0.5074867924587022\n",
      "Cost after iteration 9300: 0.49841264139840774\n",
      "Cost after iteration 9400: 0.5486269096843293\n",
      "Cost after iteration 9500: 0.5559680083770536\n",
      "Cost after iteration 9600: 0.5561297752186299\n",
      "Cost after iteration 9700: 0.5558045076099094\n",
      "Cost after iteration 9800: 0.554736048999529\n",
      "Cost after iteration 9900: 0.554195789184838\n",
      "Accuracy: 70%\n"
     ]
    }
   ],
   "source": [
    "model = L_Layer_Neural_Network([24,64,32,1])\n",
    "X_train, Y_train, X_test, Y_test = Load_Data()\n",
    "train_accuracy = model.Train(X_train, Y_train,0.0001,10000,True)\n",
    "test_accuracy = model.Performance(X_test, Y_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
